
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
df = pd.read_csv('/kaggle/input/social-network-ads/Social_Network_Ads.csv')
df.head()
#train test split
from sklearn.model_selection import train_test_split

x_train , x_test , y_train , y_test = train_test_split(df.drop('Purchased',axis=1),
                                                       df['Purchased'],test_size=0.3,random_state=0)

x_train.shape , x_test.shape
#standard scalar
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

#fit the scaler into the train data, it will learn the parameters
scaler.fit(x_train)

#transform train and test sets
x_train_scaled = scaler.transform(x_train)
x_test_scaled = scaler.transform(x_test)

scaler.mean_ #mean of age and salary
x_train_scaled
After the transformation, the data is being returned in the form of a numpy array. so, we again need to convert it to dataframe
#converting array to DataFrame

x_train_scaled = pd.DataFrame(x_train_scaled , columns = x_train.columns )
x_test_scaled = pd.DataFrame(x_test_scaled , columns = x_test.columns )
x_train_scaled #we can see that the values are scaled now.
#describing the statistics for x_train data
np.round(x_train.describe(),1)
#describing the statistics for x_train_scaled data
np.round(x_train_scaled.describe(),1)

#effect of scaling
fig, (ax1,ax2) = plt.subplots(ncols=2 , figsize=(12,5))

ax1.scatter(x_train['Age'],x_train['EstimatedSalary'])
ax1.set_title('Before Scaling')
ax2.scatter(x_train_scaled['Age'],x_train_scaled['EstimatedSalary'],color = 'red')
ax2.set_title('After Scaling')
plt.show()

Inference: We can see that the data in plot 1 has a huge scale of age and salary but in the plot 2, the data has been scaled and it has been centered to origin due to the mean being 0.
fig, (ax1,ax2) = plt.subplots(ncols=2 , figsize=(12,5))

#before scaling
ax1.set_title('Before Scaling')
sns.kdeplot(x_train['Age'],ax=ax1)
sns.kdeplot(x_train['EstimatedSalary'],ax=ax1)

#after scaling
ax2.set_title('After Scaling')
sns.kdeplot(x_train_scaled['Age'],ax=ax2)
sns.kdeplot(x_train_scaled['EstimatedSalary'],ax=ax2)

plt.show()
**Inference:** In the first plot the line is parallel because the age and salary are on different scales and they are incomparable. Whereas, after scaling the comparision between age and salary is possible and graphs are nearly the same in variaition.
#comparision of distributions
fig,(ax1, ax2) = plt.subplots(ncols=2 , figsize=(12,5))

#before scaling
ax1.set_title('Age distribution before scaling')
sns.kdeplot(x_train['Age'],ax=ax1)

#after scaling
ax2.set_title('Age distribution after scaling')
sns.kdeplot(x_train_scaled['Age'],ax=ax2)
plt.show()
#comparision of distributions
fig,(ax1, ax2) = plt.subplots(ncols=2 , figsize=(12,5))

#before scaling
ax1.set_title('salary distribution before scaling')
sns.kdeplot(x_train['EstimatedSalary'],ax=ax1)

#after scaling
ax2.set_title('salary distribution after scaling')
sns.kdeplot(x_train_scaled['EstimatedSalary'],ax=ax2)
plt.show()

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr_scaled = LogisticRegression()
lr.fit(x_train,y_train)
lr_scaled.fit(x_train_scaled,y_train)
y_pred = lr.predict(x_test)
y_pred_scaled = lr_scaled.predict(x_test_scaled)
from sklearn.metrics import accuracy_score
print('Actual',accuracy_score(y_test,y_pred))
print('Scaled',accuracy_score(y_test, y_pred_scaled))

#there is no effect of scaling on Decision Tree algorithm
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt_scaled = DecisionTreeClassifier()
dt.fit(x_train , y_train)
dt_scaled.fit(x_train_scaled , y_train)
y_pred = dt.predict(x_test)
y_pred_scaled = dt_scaled.predict(x_test_scaled)
print('Actual',accuracy_score(y_test,y_pred))
print('Scaled',accuracy_score(y_test,y_pred_scaled))

