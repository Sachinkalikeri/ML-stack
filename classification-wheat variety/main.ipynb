import numpy as np
import matplotlib.pyplot as plt 
from matplotlib import style
import pandas as pd
import seaborn as sns
import math

import torch 
from torch.optim import SGD
data = pd.read_csv('/kaggle/input/wheat-variety-classification/wheat.csv')
data = data.values
def dropout_regularizer(X, dropout=0.3):
    assert 0 <= dropout <= 1# In this case, all elements are dropped out
    if dropout == 1:
        return torch.zeros(X.shape, dtype=torch.float32)# In this case, all elements are kept
    if dropout == 0:
        return X
    mask = torch.rand(X.shape, dtype=torch.float32) > dropout
    return torch.multiply(X, mask) / (1.0 - dropout)
def relu(x):
    return torch.maximum(torch.tensor([0], dtype=torch.float32), x)

def init_params(layers):
    params = []
    for i in range(len(layers)-1):
        params.append(torch.randn((layers[i], layers[i+1]), requires_grad=True))
        params.append(torch.randn((layers[i+1]), requires_grad=True))
    return params

def xavier_initialization(layers):
    params = []
    for i in range(len(layers)-1):
        a = math.sqrt(6/(layers[i] + layers[i+1]))
        params.append(torch.tensor(np.random.uniform(-a, a, (layers[i], layers[i+1])), dtype=torch.float32, requires_grad=True))   
        #params.append(torch.Tensor((layers[i], layers[i+1]), requires_grad=True).uniform_(-a, a))
        params.append(torch.tensor(np.random.uniform(-a, a, (layers[i+1])), dtype=torch.float32, requires_grad=True))   
        #params.append(torch.randn(layers[i+1], dtype = torch.float32, requires_grad=True))
    return params

def forward(x, params, dropout = False): # we will apply dropout only when training is going on
    op = x
    for i in range(0, len(params)-2, 2):
        op = relu(torch.matmul(op, params[i]) + params[i+1])
        if dropout:
            op = dropout_regularizer(op)
    op = torch.matmul(op, params[-2]) + params[-1]
    return op # shape will be (16, 3)


def accuracy(x, y, params):
    op = forward(x, params) # shape is (210, 3)
    _, ind = torch.max(op, axis=1)
    return torch.sum(ind==y)/y.shape[0]

def pred(x, params):
    op = forward(x, params)

def train(dataset, num_epochs, lr, batch_size, params, optimizer, dropout = False, loss_fn=torch.nn.CrossEntropyLoss() ,
          print_every=3):
    acc = []
    losses =  []
    #data_x = dataset[:, :-1].clone().detach()
    #data_y = dataset[:, -1].clone().detach().to(torch.int)
    data_x = torch.tensor(dataset[:, :-1], dtype=torch.float32)
    data_y = torch.tensor(dataset[:, -1], dtype=torch.int)
    #losses.append(cross_entropy(data_y, forward(data_x, params)).item()) # this step won't be affective 
    for epoch in range(num_epochs):
        with torch.no_grad():
            acc.append(accuracy(data_x, data_y-1 , params))
        if epoch%(num_epochs//10)==0:
            print(f'Accuracy {acc[-1]}')
        total_loss = 0
        #when the dataset is larger, we can instead do the sum of all the bacthes and take their averages    
       # if epoch%(num_epochs/10)==0:
           #print(f'epoch {epoch + 1}, loss {float(losses[-1]):f}, accuracy:{accuracy(data_x, data_y-1, params)}')
        #total_loss  =0
        #batches_processed = 0
        for i, batch in  enumerate(torch.utils.data.DataLoader(dataset , batch_size=batch_size), 0):
            inp = torch.tensor(batch[:, :-1], dtype=torch.float32)
            label = torch.tensor(batch[:, -1], dtype=int)
            op = forward(inp, params, dropout)
          
             
            current_loss = loss_fn(op, label-1) # -1 for making categories 0 indexed
            total_loss+= current_loss.item()
                     
                
            optimizer.zero_grad()
            current_loss.backward()
            optimizer.step()
            
            if epoch%(num_epochs//10)==0 and i%print_every==0:
                print(f'Epoch[{epoch+1}], Current Loss: {current_loss.item()}')
            
            #batches_processed+= 1
        if epoch%(num_epochs//10)==0:
            print("\n\n")
        losses.append(total_loss/(dataset.shape[0]/batch_size)) # average loss over all batches
    return losses, acc
# hparameters and models
layer = [7,8,3]
num_epochs =1000
batch_size = 16
params2 = init_params(layer)
lr = 1e-3
optimizer2= SGD(params=params2, lr=lr)




# simple model without any hidden layer
import warnings
warnings.filterwarnings("ignore")
losses, acc = train(dataset=torch.tensor(data, dtype=torch.float32), num_epochs=num_epochs, lr=lr, 
                                batch_size=batch_size, params=params2, optimizer=optimizer2, dropout = True)            
style.use("ggplot")
plt.subplot(1,2,1)
plt.plot(range(1, num_epochs+1), losses)
plt.xlabel("Epochs ")
plt.ylabel("Losses")
#plt.legend()
plt.subplot(1,2,2)
plt.plot(range(1, num_epochs+1), acc)
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.show()
# hparameters and models
layer = [7,8,3]
num_epochs =1000
batch_size = 16
params2 = init_params(layer)
lr = 1e-3
optimizer2= SGD(params=params2, lr=lr)

# simple model without any hidden layer
import warnings
warnings.filterwarnings("ignore")
losses, acc = train(dataset=torch.tensor(data, dtype=torch.float32), num_epochs=num_epochs, lr=lr, 
                                batch_size=batch_size, params=params2, optimizer=optimizer2, dropout = False)            
style.use("ggplot")
plt.subplot(1,2,1)
plt.plot(range(1, num_epochs+1), losses)
plt.xlabel("Epochs ")
plt.ylabel("Losses")
#plt.legend()
plt.subplot(1,2,2)
plt.plot(range(1, num_epochs+1), acc)
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.show()
