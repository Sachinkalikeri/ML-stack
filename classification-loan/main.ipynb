
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.simplefilter("ignore")
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
df=pd.read_csv("/kaggle/input/eligibility-prediction-for-loan/Loan_Data.csv")
df.head()
print(df.shape)
df.describe().T
df.drop(columns=['Loan_ID'],inplace=True)
df.dtypes
cat_columns=[]
num_columns=[]
for col in df.columns:
    if df[col].dtypes=="object":
        cat_columns.append(col)
    else:
        num_columns.append(col)
cat_columns.remove('Loan_Status')        
print("categorical Columns:", cat_columns)
num_columns.remove('Credit_History')
print("Numerical Columns:",num_columns)
df['Loan_Status'].value_counts(normalize=True)
As your dataset has 31% of the target variable as the minority class, then it is relatively well-balanced and it may not be necessary to perform upsampling techniques.
df.isnull().mean().sort_values(ascending=False)*100
 We should perform preprocessing after splitting the data into training, validation, and testing sets to avoid data leakage.
from sklearn.model_selection import train_test_split
train,test=train_test_split(df,test_size=0.2,random_state=123)
train['Source']='Train'
test['Source']='Test'
print(train.shape)
print(test.shape)
print("TRAIN")
print(train.isnull().mean().sort_values(ascending=False)*100)
print("-----------------------------------------------------")
print("-----------------------------------------------------")
print("TEST")
print(test.isnull().mean().sort_values(ascending=False)*100)
train=train[train['Credit_History'].notnull()]
test=test[test['Credit_History'].notnull()]
# Here we are filling missing values with mode and mean because missing data is less than 5%. Also if we want we can directly use Imputer from sklearn rather than doing this manually.
train['Married'].fillna(train['Married'].mode()[0],inplace=True)
test['Married'].fillna(train['Married'].mode()[0],inplace=True)
train['Gender'].fillna(train['Gender'].mode()[0],inplace=True)
test['Gender'].fillna(train['Gender'].mode()[0],inplace=True)
train['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].median(),inplace=True)
test['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].median(),inplace=True)
train['Dependents'].fillna(train['Dependents'].mode()[0],inplace=True)
test['Dependents'].fillna(train['Dependents'].mode()[0],inplace=True)
train['LoanAmount'].fillna(train['LoanAmount'].median(),inplace=True)
test['LoanAmount'].fillna(train['LoanAmount'].median(),inplace=True)
train['Self_Employed'].fillna(train['Self_Employed'].mode()[0],inplace=True)
test['Self_Employed'].fillna(train['Self_Employed'].mode()[0],inplace=True)
print("TRAIN")
print(train.isnull().mean().sort_values(ascending=False)*100)
print("-----------------------------------------------------")
print("-----------------------------------------------------")
print("TEST")
print(test.isnull().mean().sort_values(ascending=False)*100)
# EDA
train.dtypes
def plot_countplots(train, x_cols, y_col):
    n_cols = len(x_cols)
    fig, axes = plt.subplots(1, n_cols, figsize=(5*n_cols,5), sharey=True)
    for i, col in enumerate(x_cols):
        sns.countplot(data=train, x=col, hue=y_col, ax=axes[i])
        axes[i].set_title(col)
    plt.show()
plot_countplots(train,cat_columns,'Loan_Status')
import matplotlib.pyplot as plt
def plot_boxplots(train, x_cols, y_col):
    n_cols = len(x_cols)
    fig, axes = plt.subplots(1, n_cols, figsize=(5*n_cols,5), sharey=True)
    for i, col in enumerate(x_cols):
        sns.boxplot(data=train, x=col, y=y_col, ax=axes[i])
        axes[i].set_title(col)
    plt.show()
plot_boxplots(train,num_columns,'Loan_Status')
def draw_distplots(df, columns_list):
    fig, axes = plt.subplots(nrows=1, ncols=len(columns_list), figsize=(15, 5))
    
    for i, col in enumerate(columns_list):
        sns.distplot(df[col], ax=axes[i])
        axes[i].set_title(f"Countplot of {col}")
    
    plt.tight_layout()
    plt.show()
draw_distplots(df=train,columns_list=num_columns)
skew=[]
for col in num_columns:
    skew.append(col)
    skew.append(train[col].skew())
    skew.append(np.log1p(train[col]).skew())
    skew.append("------------")
skew
from sklearn.preprocessing import FunctionTransformer
Func=FunctionTransformer(func=np.log1p)
train['ApplicantIncome']=Func.fit_transform(train['ApplicantIncome'])
test['ApplicantIncome']=Func.transform(test['ApplicantIncome'])
from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer(method='yeo-johnson',standardize=False)
train[['Loan_Amount_Term','LoanAmount','CoapplicantIncome']]=pt.fit_transform(train[['Loan_Amount_Term','LoanAmount','CoapplicantIncome']])
test[['Loan_Amount_Term','LoanAmount','CoapplicantIncome']]=pt.transform(test[['Loan_Amount_Term','LoanAmount','CoapplicantIncome']])
draw_distplots(df=train,columns_list=num_columns)
from sklearn.preprocessing import StandardScaler
Scaling=StandardScaler()
train[['Loan_Amount_Term','LoanAmount','CoapplicantIncome','ApplicantIncome']]=Scaling.fit_transform(train[['Loan_Amount_Term','LoanAmount','CoapplicantIncome','ApplicantIncome']])
test[['Loan_Amount_Term','LoanAmount','CoapplicantIncome','ApplicantIncome']]=Scaling.transform(test[['Loan_Amount_Term','LoanAmount','CoapplicantIncome','ApplicantIncome']])
Fullraw=pd.concat([train,test],axis=0)
Fullraw
# Unbalaced Data
Fullraw['Loan_Status'].value_counts().plot(kind='pie')
Fullraw2=pd.get_dummies(Fullraw,drop_first=True)
Fullraw2.shape
Fullraw2
trainX=Fullraw2[Fullraw2['Source_Train']==1].drop(columns=['Source_Train','Loan_Status_Y'])
testX=Fullraw2[Fullraw2['Source_Train']==0].drop(columns=['Source_Train','Loan_Status_Y'])
trainY=Fullraw2[Fullraw2['Source_Train']==1]['Loan_Status_Y']
testY=Fullraw2[Fullraw2['Source_Train']==0][['Loan_Status_Y']]
# UpSampling
from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=42)
X_train_resampled, y_train_resampled = ros.fit_resample(trainX, trainY)
print(X_train_resampled.shape)
print(y_train_resampled.value_counts().plot(kind='pie'))
# Models
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import recall_score,precision_score,accuracy_score,f1_score
svc = SVC()
knc = KNeighborsClassifier()
dtc = DecisionTreeClassifier(max_depth=3,min_samples_leaf = 35)
lrc = LogisticRegression()
rfc = RandomForestClassifier(n_estimators=100,max_depth=3,min_samples_leaf = 10)
abc = AdaBoostClassifier()
bc = BaggingClassifier()
etc = ExtraTreesClassifier()
gbdt = GradientBoostingClassifier()
xgb = XGBClassifier()
clfs = {
    'SVC' : svc,
    'KN' : knc, 
    'DT': dtc, 
    'LR': lrc, 
    'RF': rfc, 
    'AdaBoost': abc, 
    'BaggingClassifier': bc, 
    'ETC': etc,
    'GradientBoosting':gbdt
}
def train_classifier(clf,X_train_resampled,y_train_resampled,testX,testY):
    clf.fit(X_train_resampled,y_train_resampled)
    y_pred = clf.predict(testX)
    accuracy = accuracy_score(testY,y_pred)
    precision = precision_score(testY,y_pred)
    recall=recall_score(testY,y_pred)
    f1=f1_score(testY,y_pred)
    
    
    return accuracy,precision,recall,f1
accuracy_scores = []
precision_scores = []
recal_scores=[]
f1_scores=[]

for name,clf in clfs.items():
    
    current_accuracy,current_precision,current_recall,current_f1 = train_classifier(clf, X_train_resampled,y_train_resampled,testX,testY)
    
    print("For ",name)
    print("Accuracy - ",current_accuracy)
    print("Precision - ",current_precision)
    print("Recall - ",current_recall)
    print("f1 - ",current_f1)
    
    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)
    recal_scores.append(current_recall)
    f1_scores.append(current_f1)
performance_df=pd.DataFrame({'Algorithm':clfs.keys(),'f1_score':f1_scores,'Precision':precision_scores,'recall_score':recal_scores,'accuracy':accuracy_scores}).sort_values('f1_score',ascending=False)
performance_df1=pd.melt(performance_df, id_vars = "Algorithm")
performance_df
sns.catplot(x = 'Algorithm', y='value', 
               hue = 'variable',data=performance_df1, kind='bar',height=5,aspect=1.3)
plt.ylim(0.5,1.0)
plt.xticks(rotation='vertical')
plt.show()
