import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

df=pd.read_csv('/kaggle/input/easiest-diabetes-classification-dataset/Diabetes Classification.csv')
df.head(2)
df.info()
There is no missing values
df.describe()
catcol=df.select_dtypes(include=['object']).columns.tolist()
valcol = df.select_dtypes(exclude=['object']).columns.tolist()

cat=df[catcol]
val=df[valcol]
#val['Diagnosis']=df['Diagnosis']
val.dtypes
We will plot every object category with the Diagnosis to see if there is some corrolation
fig, axes = plt.subplots(2, 3, figsize=(12, 8))

# Flatten the axes array for easier iteration
axes = axes.flatten()

# Iterate over the subplots
for i, ax in enumerate(axes):
    # Generate a countplot for each subplot
    sns.countplot(data=cat, x=cat.columns[i], ax=ax,hue='Diagnosis')
    ax.set_title(cat.columns[i])  # Set subplot title

# Adjust the layout and spacing
plt.tight_layout()

# Display the subplots
plt.show()
The number of entries is low so it is hard to take real intuitions of the data.
It is unlogical that there is more people that have diabetes and there family don't have a history of diabetes
from sklearn.preprocessing import LabelEncoder
BloodOrder=['Low','Normal','High']
FamHis=['No','Yes']
SmoHis=['No','Yes']
DietOr=['Poor','Healthy']
ExcerOr=['No','Regular']

label_encoder = LabelEncoder()
label_encoder.fit(BloodOrder)
df['Blood Pressure'] = label_encoder.transform(df['Blood Pressure'])

label_encoder.fit(FamHis)
df['Family History of Diabetes']=label_encoder.transform(df['Family History of Diabetes'])

label_encoder.fit(SmoHis)
df['Smoking']=label_encoder.transform(df['Smoking'])

label_encoder.fit(DietOr)
df['Diet']=label_encoder.transform(df['Diet'])

df['Exercise'].replace({'No':0,'Regular':1},inplace=True)
df['Diagnosis'].replace({'No':0,'Yes':1},inplace=True)
encoded_features = pd.get_dummies(df['Gender'], prefix='Gender')

# Replace the original column with the encoded features
df = pd.concat([df.drop('Gender', axis=1), encoded_features], axis=1)
df
##Numeric Values
We will plot a boxplot for all the numeric values
# Create subplots with 2 rows and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(10, 8))

# Flatten the axes array for easier iteration
axes = axes.flatten()

# Iterate over the columns and subplots
for i, (col, ax) in enumerate(zip(val.columns, axes)):
    # Plot boxplot for each column
    ax.boxplot(val[col])
    ax.set_title(col)

# Adjust spacing
plt.tight_layout()

# Display the plot
plt.show()
We can see there is no outliers and the data are well distributed around the mean
# Create subplots with 2 rows and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(10, 8))

# Flatten the axes array for easier iteration
axes = axes.flatten()

# Iterate over the columns and subplots
for i, (col, ax) in enumerate(zip(val.columns, axes)):
    # Plot histogram for each column
    sns.histplot(data=val, x=col, ax=ax)
    ax.set_title(col)

# Adjust spacing
plt.tight_layout()

# Display the plot
plt.show()
Bcox = pd.DataFrame()
for col in val.columns:
    transformed_data, _ = stats.boxcox(df[col] + 1)  # Adding 1 to handle zero and negative values
    Bcox[col] = transformed_data

# Create subplots with 2 rows and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(10, 8))

# Flatten the axes array for easier iteration
axes = axes.flatten()

# Iterate over the columns and subplots
for i, (col, ax) in enumerate(zip(Bcox.columns, axes)):
    # Plot histogram for each column
    sns.histplot(Bcox[col], ax=ax)
    ax.set_title(col + ' (Box-Cox transformed)')

# Adjust spacing
plt.tight_layout()

# Display the plot
plt.show()
# Create subplots with 2 rows and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(10, 8))
log=np.log(val)
# Flatten the axes array for easier iteration
axes = axes.flatten()

# Iterate over the columns and subplots
for i, (col, ax) in enumerate(zip(log.columns, axes)):
    # Plot histogram for each column
    sns.histplot(log[col], ax=ax)
    ax.set_title(col + ' (LOg)')

# Adjust spacing
plt.tight_layout()

# Display the plot
log.skew()
Bcox.skew()
df[Bcox.columns]=Bcox
By observing visually the distribution of the 2 transformations and by seeing that the skewness of the Bcox is lower than the log transformation, i'll take the Bcox transformation

d=df['Diagnosis'].replace({'No':0,'Yes':1})
for x in val.columns:
  tmp=Bcox[x].corr(d)
  print('Corr ' ,x ,' with Diagnosis=',tmp)

There isn't much difference between the corrolation of different variables so i will keep them all
df.columns
from sklearn.model_selection import train_test_split

# Assuming your data is stored in a DataFrame called 'data'
X = df.drop('Diagnosis', axis=1)  # Features (input variables)
y = df['Diagnosis']  # Target variable (output variable)

# Split the data into train, validation, and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Print the sizes of each set
print("Train set size:", X_train.shape[0])
print("Test set size:", X_test.shape[0])

df[valcol]
from sklearn.preprocessing import StandardScaler

# Create a copy of the original X_train dataset
X_train_scaled = X_train.copy()
X_test_scaled=X_test.copy()
# Select only the columns in 'cols'
X_train_to_scale = X_train[valcol]
X_test_to_scale = X_test[valcol]

# Create an instance of the StandardScaler
scaler = StandardScaler()
scaler.fit(X_train_to_scale)
# Fit and transform the selected columns
X_train_scaled[valcol] = scaler.transform(X_train_to_scale)
X_test_scaled[valcol]=scaler.transform(X_test_to_scale)
# Continue using the rest of the columns as they are

from sklearn.svm import SVC
svm_model = SVC()
svm_model.fit(X_train_scaled,y_train)
ypred=svm_model.predict(X_train_scaled)
YtestPred=svm_model.predict(X_test_scaled)
from sklearn.metrics import accuracy_score

# Assuming you have true labels 'y_true' and predicted labels 'y_pred'

# Calculate the accuracy score
Tacc = accuracy_score(y_train, ypred)
Te = accuracy_score(y_test,YtestPred)

# Print the accuracy score
print("Accuracy:", Tacc)
print("Test Accuracy:",Te)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, YtestPred)
ax = sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")

# Customize the plot
plt.xlabel("Predicted Labels")
plt.ylabel("Actual Labels")
plt.title("Confusion Matrix")

# Show the plot
plt.show()


