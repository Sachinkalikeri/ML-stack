# Importing the necessary libraries
import pandas as pd
import numpy as np
import sklearn
from scipy import stats
import matplotlib.pyplot as plt
import os
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score
# Load the dataset
data = pd.read_csv('/kaggle/input/happiness-classification-dataset/happydata.csv')
data.head()
data.tail()
# Explore the dataset
print("Dataset shape:", data.shape)
print("Columns in the dataset:", data.columns)
# Check for duplicates
print("\nNumber of duplicate rows:", data.duplicated().sum())
# Handle duplicates
data.drop_duplicates(inplace=True)
# Updated dataset after handling missing values and duplicates
print("\nDataset shape after handling missing values and duplicates:", data.shape)
data.dtypes
# Define the z-score threshold for outlier detection
z_score_threshold = 3

# Loop through each column and detect outliers
for column in data.columns:
    z_scores = np.abs(stats.zscore(data[column]))
    outliers = np.where(z_scores > z_score_threshold)
    
    print("Column:", column)
    print("Number of outliers:", len(outliers[0]))
    print("Indices of outliers:", outliers[0])
    print()
# Define the z-score threshold for outlier detection
z_score_threshold = 3

# Create a copy of the dataset
data_no_outliers = data.copy()

# Loop through each column and remove outliers
for column in data.columns:
    z_scores = np.abs(stats.zscore(data[column]))
    outliers = np.where(z_scores > z_score_threshold)
    
    # Remove outliers from the copy of the dataset
    data_no_outliers = data_no_outliers.drop(outliers[0])
# Create box plots for each column
plt.figure(figsize=(12, 6))
plt.boxplot(data.values, labels=data.columns)
plt.title('Box Plot - Outlier Detection')
plt.xlabel('Columns')
plt.ylabel('Values')
plt.show()
# Create scatter plots for each column
plt.figure(figsize=(12, 6))
for i, column in enumerate(data.columns):
    plt.subplot(2, 4, i+1)
    plt.scatter(range(len(data)), data[column])
    plt.title(column)
    plt.xlabel('Index')
    plt.ylabel('Value')
plt.tight_layout()
plt.show()
# Calculate descriptive statistics
data_no_outliers.describe()
# Create histograms for each variable
data_no_outliers.hist(figsize=(10, 6))
plt.tight_layout()
plt.show()
# Calculate the correlation matrix
correlation_matrix = data_no_outliers.corr()

# Create a heatmap of the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm")
plt.title("Correlation Matrix")
plt.show()
correlation_matrix
cat_vars = ['infoavail', 'schoolquality', 'policetrust', 'streetquality', 'Ã«vents']
for var in cat_vars:
    plt.figure(figsize=(8, 6))
    data[var].value_counts().plot(kind='bar')
    plt.title(var)
    plt.xlabel('Categories')
    plt.ylabel('Count')
    plt.show()
num_vars = ['housecost']
for var in num_vars:
    plt.figure(figsize=(8, 6))
    data.boxplot(column=var)
    plt.title(var)
    plt.ylabel('Value')
    plt.show()
plt.figure(figsize=(8, 6))
plt.scatter(data['housecost'], data['happy'])
plt.title('House Cost vs. Happiness')
plt.xlabel('House Cost')
plt.ylabel('Happiness')
plt.show()
# Create pair plots
sns.pairplot(data_no_outliers)
plt.show()
# Bar plots
plt.figure(figsize=(8, 6))
data.groupby('happy').mean().plot(kind='bar')
plt.title('Average Happiness Level')
plt.xlabel('Happy (0: Unhappy, 1: Happy)')
plt.ylabel('Average Value')
plt.legend(loc='upper center')
plt.xticks(rotation=0)
plt.show()
# Compare 'happy' values based on 'infoavail' levels
sns.boxplot(x='infoavail', y='happy', data=data_no_outliers)
plt.title("'happy' Values by 'infoavail' Levels")
plt.show()
# Split the dataset into features (X) and target variable (y)
X = data_no_outliers.drop('happy', axis=1)
y = data_no_outliers['happy']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Initialize the logistic regression model
model = LogisticRegression()

# Fit the model to the training data
model.fit(X_train, y_train)

# Predict the target variable for the test data
y_pred = model.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
# Calculate precision
precision = precision_score(y_test, y_pred)

# Calculate recall
recall = recall_score(y_test, y_pred)

# Calculate F1-score
f1 = f1_score(y_test, y_pred)

print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize a random forest classifier
model = RandomForestClassifier()

# Fit the model to the training data
model.fit(X_train, y_train)

# Predict the target variable for the test data
y_pred = model.predict(X_test)

# Calculate precision, recall, and F1-score
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
# Define the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}

# Perform grid search for hyperparameter tuning
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Get the best parameters and best model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Perform cross-validation with the best model
cv_scores = cross_val_score(best_model, X, y, cv=5)

# Get feature importance
feature_importance = best_model.feature_importances_

print("Best Parameters:", best_params)
print("Cross-Validation Scores:", cv_scores)
print("Feature Importance:", feature_importance)
# Evaluate the best model on the test set
y_pred_test = best_model.predict(X_test)

# Calculate evaluation metrics on the test set
accuracy = accuracy_score(y_test, y_pred_test)
precision = precision_score(y_test, y_pred_test)
recall = recall_score(y_test, y_pred_test)
f1 = f1_score(y_test, y_pred_test)

print("Test Set Accuracy:", accuracy)
print("Test Set Precision:", precision)
print("Test Set Recall:", recall)
print("Test Set F1-score:", f1)