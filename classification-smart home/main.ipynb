import numpy as np
import pandas as pd
import os
import seaborn as sns
import matplotlib.pyplot as plt

%matplotlib inline

for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import numpy as np 
import pandas as pd # data processing, CSV file

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
data = pd.read_csv("/kaggle/input/smart-home-commands-dataset/dataset.csv")
del data["Number"]

data.sample(frac=1).head(10)

data.info()
data.shape
non_null_counts = data.count()

print(non_null_counts)
plt.figure(figsize=(18,10))
sns.countplot(x="Category", palette="rocket", data=data)
plt.figure(figsize=(18,10))
sns.countplot(x="Subcategory", palette="rocket", data=data)
plt.figure(figsize=(10,5))
sns.countplot(x="Question", palette="rocket", data=data)
plt.figure(figsize=(15,10))
sns.countplot(x="Time", palette="rocket", data=data)
plt.figure(figsize=(15,10))
sns.countplot(x="Action", palette="rocket", data=data)
from nltk import word_tokenize
from sklearn.model_selection import train_test_split
import itertools
import math
sentences = data['Sentence']
categories = data['Category']
subcategories = data['Subcategory']
actions = data['Action']

uniquecategories = list(set(categories))
uniquesubcategories = list(set(subcategories))
uniqueactions = list(set(actions))

mergesentences = list(itertools.chain.from_iterable([word_tokenize(sentence.lower()) for sentence in sentences]))
vocabulary = list(set(mergesentences))
print(vocabulary)

def term_frequency(word, sentence):
    return sentence.split().count(word)
def document_frequency(word):
    return vocabulary.count(word)
def inverse_document_frequency(word):
    return math.log(len(vocabulary) / (document_frequency(word) + 1))
def calculate_tfidf(word, sentence):
    return term_frequency(word, sentence) * inverse_document_frequency(word)
def one_hot_class_vector(uniqueclasses, w):
    empvec = [0 for i in range(len(uniqueclasses))]
    empvec[uniqueclasses.index(w)] = 1
    return empvec
def one_hot_vector(w):
    empvec = [0 for i in range(len(vocabulary))]
    empvec[vocabulary.index(w)] = 1
    return empvec
def sentence_vector(sentence, tfidf=False):
    tokenizedlist = word_tokenize(sentence.lower())
    sentencevector = [0 for i in range(len(vocabulary))]
    count = 0

    for word in tokenizedlist:
        if word in vocabulary:
            count = count + 1
            if tfidf:
                sentencevector = [x + y for x, y in zip(sentencevector, [e * calculate_tfidf(word, sentence) for e in one_hot_vector(word)])] 
            else:
                sentencevector = [x + y for x, y in zip(sentencevector, one_hot_vector(word))]

    if count == 0:
        return sentencevector
    else:
        return [(el / count) for el in sentencevector]

categoryvectors = [cv.index(1) for cv in [one_hot_class_vector(uniquecategories, w) for w in categories]]
subcategoryvectors = [cv.index(1) for cv in [one_hot_class_vector(uniquesubcategories, w) for w in subcategories]]
actionvectors = [cv.index(1) for cv in [one_hot_class_vector(uniqueactions, w) for w in actions]]
sentencevectors = [sentence_vector(sentence) for sentence in sentences]
sentencevectorstfidf = [sentence_vector(sentence, True) for sentence in sentences]
X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(sentencevectors, categoryvectors, test_size=0.25, random_state=42)
X_train_cat_tfidf, X_test_cat_tfidf, y_train_cat_tfidf, y_test_cat_tfidf = train_test_split(sentencevectorstfidf, categoryvectors, test_size=0.25, random_state=42)
X_train_subcat, X_test_subcat, y_train_subcat, y_test_subcat = train_test_split(sentencevectors, subcategoryvectors, test_size=0.25, random_state=42)
X_train_action, X_test_action, y_train_action, y_test_action = train_test_split(sentencevectors, actionvectors, test_size=0.25, random_state=42)
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from catboost import CatBoostClassifier
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
import xgboost as xgb
from tpot import TPOTClassifier
from sklearn.metrics import accuracy_score
from numpy import random

random.seed(2020)
def train_fit(model_name, model, X, y, X_test, y_test):
    model.fit(X, y)
    y_preds = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_preds)
    print(f"{model_name}: {accuracy}")
    return model
random_forest_model = RandomForestClassifier()
random_forest_model = train_fit("RandomForestClassifier", random_forest_model, X_train_cat, y_train_cat, X_test_cat, y_test_cat)
mlp_max_iter_model = MLPClassifier(max_iter=10000)
mlp_max_iter_model = train_fit("MLPClassifier", mlp_max_iter_model, X_train_cat, y_train_cat, X_test_cat, y_test_cat)
mlp_max_iter_model_cat = MLPClassifier(max_iter=10000)
mlp_max_iter_model_cat = train_fit("MLPClassifier", mlp_max_iter_model_cat, X_train_cat, y_train_cat, X_test_cat, y_test_cat)
mlp_max_iter_model_subcat = MLPClassifier(max_iter=10000)
mlp_max_iter_model_subcat = train_fit("MLPClassifier", mlp_max_iter_model_subcat, X_train_subcat, y_train_subcat, X_test_subcat, y_test_subcat)
mlp_max_iter_model_action = MLPClassifier(max_iter=10000)
mlp_max_iter_model_action = train_fit("MLPClassifier", mlp_max_iter_model_action, X_train_action, y_train_action, X_test_action, y_test_action)
def predict(model, classes, sentence):
    y_preds = model.predict([sentence_vector(sentence)])
    return classes[y_preds[0]]

sentence = "Hi Google, please turn off the lights."
print(predict(mlp_max_iter_model, uniquecategories, sentence))
print(predict(mlp_max_iter_model_subcat, uniquesubcategories, sentence))
print(predict(mlp_max_iter_model_action, uniqueactions, sentence))
