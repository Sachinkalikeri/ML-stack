# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

#
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

job_placement = pd.read_csv("/kaggle/input/job-placement-dataset/Job_Placement_Data.csv")
records = pd.DataFrame(data = job_placement)
# DataFrame "records" will contain the dataset information. If any of the code blocks are gonna modify "records", the block abbove should be reinitialised
records.head()
# Identify the attribute types:
records.dtypes
from sklearn.preprocessing import MinMaxScaler
# Identify columns with float or int types
numeric_columns = records.select_dtypes(include=['float']).columns
Scalated_records = records.copy()

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Scale numeric columns between 0 and 1
Scalated_records[numeric_columns] = scaler.fit_transform(Scalated_records[numeric_columns])
Scalated_records.head(10)


from sklearn.preprocessing import StandardScaler
numeric_colums = records.select_dtypes(include = ['float']).columns

# Create a copy of the DataFrame
Standard_records = Scalated_records.copy()
# Initialise the Standard Scaller
scaler = StandardScaler()
# Create the DF with standardised values:
Standard_records[numeric_columns] = scaler.fit_transform(Standard_records[numeric_colums])

Standard_records.head()
import matplotlib.pyplot as plt

plt.plot(Standard_records.index, Standard_records.ssc_percentage)
plt.show()
from sklearn.preprocessing import KBinsDiscretizer

numeric_colums = records.select_dtypes(include = ['float']).columns
discretizer = KBinsDiscretizer(n_bins = 5, encode = "ordinal", strategy = "uniform")

discretized_values = discretizer.fit_transform(records[numeric_columns])

discretized_df = pd.DataFrame(discretized_values, columns=numeric_columns)

plt.plot(discretized_df.index,discretized_df.hsc_percentage,color = "g")
import random
import numpy as np

records.loc[random.sample(range(0, 215), 10), 'ssc_percentage'] = np.nan

records.isna().sum().sum()
from matplotlib import pyplot

dependence = pyplot.scatter(records.ssc_percentage, records.mba_percent)

dependence = pyplot.scatter(records.ssc_percentage, records.mba_percent)
r = np.corrcoef(records.ssc_percentage, records.hsc_percentage)
r
r = np.corrcoef(records.ssc_percentage, records.mba_percent)
r
from sklearn.impute import KNNImputer

knn_imputer = KNNImputer(n_neighbors = 3)
records.loc[:,['ssc_percentage']] = knn_imputer.fit_transform(records.loc[:,['ssc_percentage']])

records.isna().sum().sum()
import numpy as np

def gini(x):
    total = 0
    for i, xi in enumerate(x[:-1], 1):
        total += np.sum(np.abs(xi - x[i:]))
    return total / (len(x)**2 * np.mean(x))
ginis = []
ginis.append(gini(np.array(records.ssc_percentage)))
ginis.append(gini(np.array(records.hsc_percentage)))
ginis.append(gini(np.array(records.degree_percentage)))
ginis.append(gini(np.array(records.emp_test_percentage)))
ginis.append(gini(np.array(records.mba_percent)))

numerical_columns = ['ssc_percentage','hsc_percentage','degree_percentage','emp_test_percentage','mba_percent']
d = {'numerical_columns': numerical_columns, 'gini_index': ginis}
Gini_relevances = pd.DataFrame(data = d)
Gini_relevances.sort_values('gini_index', inplace = True)
Gini_relevances
