import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
Load Dataset
df = pd.read_csv('/kaggle/input/car-acceptability-classification-dataset/car.csv')
df.head()
df.shape
df.info()
Checking Unique values in categorical columns
df['Buying_Price'].unique()
df['Maintenance_Price'].unique()
df['Size_of_Luggage'].unique()
df['Safety'].unique()
df['Car_Acceptability'].unique()
**Converting Categorical variables into numeric values**
from sklearn.preprocessing import LabelEncoder
LE = LabelEncoder()
df['Buying_Price'] = LE.fit_transform(df['Buying_Price'])
df['Maintenance_Price'] = LE.fit_transform(df['Maintenance_Price'])
df['Size_of_Luggage'] = LE.fit_transform(df['Size_of_Luggage'])
df['Safety'] = LE.fit_transform(df['Safety'])
df['Car_Acceptability'] = LE.fit_transform(df['Car_Acceptability'])
df.dtypes
df['No_of_Doors'] = LE.fit_transform(df['No_of_Doors'])
df['Person_Capacity'] = LE.fit_transform(df['Person_Capacity'])
df.head()
df['No_of_Doors'].astype(int)
**Data Visualization**
Bar Plot:
car_counts = df['Car_Acceptability'].value_counts()
plt.bar(car_counts.index, car_counts.values)
plt.title('Car Acceptability Distribution')
plt.xlabel('Car Acceptability')
plt.ylabel('Count')

plt.show()

Scatter Plot:
plt.scatter(df['Buying_Price'], df['Maintenance_Price'])
plt.title('Buying Price vs. Maintenance Price')
plt.xlabel('Buying Price')
plt.ylabel('Maintenance Price')

plt.show()

Histogram:
plt.hist(df['Person_Capacity'], bins=5)
plt.title('Person Capacity Distribution')
plt.xlabel('Person Capacity')
plt.ylabel('Frequency')

plt.show()

**Train & Test Split**
X = df.iloc[:,:-1]
X
y = df.iloc[:,-1:]
y
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)
**DecisionTreeClassifier**
from sklearn.tree import DecisionTreeClassifier
DT = DecisionTreeClassifier()
from sklearn.model_selection import cross_val_score
from hyperopt import hp,tpe,fmin,STATUS_OK,Trials
Space = {
    'criterion' : hp.choice('criterion',["gini", "entropy", "log_loss"]),
    'splitter' : hp.choice('splitter',["best", "random"]),
    'max_depth' : hp.quniform('max_depth',1,10,1)
}
def Bayesian(Space):
  DT = DecisionTreeClassifier(
      criterion = Space['criterion'],
      splitter = Space['splitter'],
      max_depth = int(Space['max_depth'])
  )
  accuracy = cross_val_score(DT,X_train,y_train,cv=5).mean()
  return{'loss' : -accuracy , 'status' : STATUS_OK}
trials = Trials()
Best = fmin(fn=Bayesian,space=Space,algo=tpe.suggest,max_evals=200,trials=trials)
Best
DT = DecisionTreeClassifier(criterion = 'log_loss',
                            max_depth = 10,)
DT.fit(X_train,y_train)
Making Prediction using our model
y_hat = DT.predict(X_test)
**Model Evaluation**
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print(accuracy_score(y_test,y_hat))
print(classification_report(y_test,y_hat))
print(confusion_matrix(y_test,y_hat))
**Plotting DecisionTree**
from sklearn import tree
fig, ax = plt.subplots(figsize=(50, 50))
tree.plot_tree(DT, ax=ax)
plt.show()
